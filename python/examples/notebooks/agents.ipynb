{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeeAI ReAct agents\n",
    "\n",
    "The BeeAI ReAct agent is a pre-configured implementation of the ReAct (Reasoning and Acting) pattern. It can be customized with tools and instructions to suit different tasks.\n",
    "\n",
    "The ReAct pattern is a framework used in AI models, particularly language models, to separate the reasoning process from the action-taking process. This pattern enhances the model's ability to handle complex queries by enabling it to:\n",
    "\n",
    "- Reason about the problem.\n",
    "- Decide on an action to take.\n",
    "- Observe the result of that action to inform further reasoning and actions.\n",
    "\n",
    "The ReAct agent provides a convenient out-of-the-box agent implementation that makes it easier to build agents using this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ReAct Agent\n",
    "\n",
    "To configure a ReAct agent, you need to define a ChatModel and construct a BeeAgent.\n",
    "\n",
    "In this example, we won't provide any external tools to the agent. It will rely solely on its own memory to provide answers. This is a basic setup where the agent tries to reason and act based on the context it has built internally.\n",
    "\n",
    "Try modifying the input text in the call to agent.run() to experiment with obtaining different answers. This will help you see how the agent's reasoning and response vary with different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A water molecule consists of two hydrogen atoms and one oxygen atom. This can be represented as H2O.\n"
     ]
    }
   ],
   "source": [
    "from beeai_framework.agents.bee.agent import BeeAgent\n",
    "from beeai_framework.agents.types import BeeInput, BeeRunInput, BeeRunOutput\n",
    "from beeai_framework.backend.chat import ChatModel\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "\n",
    "# Construct ChatModel\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "\n",
    "# Construct Agent instance with the chat model\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[], memory=UnconstrainedMemory()))\n",
    "\n",
    "# Run the agent\n",
    "result: BeeRunOutput = await agent.run(run_input=BeeRunInput(prompt=\"What chemical elements make up a water molecule?\"))\n",
    "\n",
    "print(result.result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tools\n",
    "\n",
    "To go beyond just chatting with an LLM, you can provide tools to the agent. This enables the agent to perform specific tasks and interact with external systems, enhancing its functionality. There are different ways to add tools to the agent:\n",
    "\n",
    "- Built-in tools from the framework: BeeAI provides several built-in tools that you can easily integrate with your agent.\n",
    "- Importing tools from other libraries: You can bring in external tools or APIs to extend your agent's capabilities.\n",
    "- Custom tooling: You can also write your own custom tools tailored to your specific use case.\n",
    "- \n",
    "By equipping the agent with these tools, you allow it to perform more complex actions, such as querying databases, interacting with APIs, or manipulating data.\n",
    "\n",
    "## Built-in tools\n",
    "\n",
    "BeeAI comes with several built-in tools that are part of the library, which can be easily imported and added to your agent.\n",
    "\n",
    "In this example, we provide the agent with a weather forecast lookup tool called OpenMeteoTool. With this tool, the agent can retrieve real-time weather data, enabling it to answer weather-related queries with more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in London is 8.2 degrees Celsius with no rainfall, relative humidity of 85%, and a wind speed of 7.6 km/h.\n"
     ]
    }
   ],
   "source": [
    "from beeai_framework.backend.chat import ChatModel\n",
    "from beeai_framework.tools.weather.openmeteo import OpenMeteoTool\n",
    "\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "\n",
    "# create an agent using the default LLM and add the OpenMeteoTool that is capable of fetching weather-based information\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[OpenMeteoTool()], memory=UnconstrainedMemory()))\n",
    "\n",
    "result: BeeRunOutput = await agent.run(run_input=BeeRunInput(prompt=\"What's the current weather in London?\"))\n",
    "\n",
    "print(result.result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tools\n",
    "\n",
    "You can also write your own custom tools from scratch and integrate them into your agent.\n",
    "\n",
    "To define a custom tool, simply use the @tool decorator on your function. Then, pass the tool into your agent just like you would with built-in tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square root of 36 is 6.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from urllib.parse import quote\n",
    "\n",
    "import requests\n",
    "\n",
    "from beeai_framework.agents.bee.agent import BeeAgent\n",
    "from beeai_framework.backend.chat import ChatModel\n",
    "from beeai_framework.tools import tool\n",
    "from beeai_framework.tools.tool import StringToolOutput\n",
    "from beeai_framework.tools.weather.openmeteo import OpenMeteoTool\n",
    "\n",
    "\n",
    "# defining a tool using the `tool` decorator\n",
    "# Note: the pydoc is important as it serves as the tool description to the agent\n",
    "@tool\n",
    "def basic_calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    A calculator tool that performs mathematical operations.\n",
    "\n",
    "    Args:\n",
    "        expression: The mathematical expression to evaluate (e.g., \"2 + 3 * 4\").\n",
    "\n",
    "    Returns:\n",
    "        The result of the mathematical expression\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoded_expression = quote(expression)\n",
    "        math_url = f\"https://newton.vercel.app/api/v2/simplify/{encoded_expression}\"\n",
    "\n",
    "        response = requests.get(\n",
    "            math_url,\n",
    "            headers={\"Accept\": \"application/json\"},\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return StringToolOutput(json.dumps(response.json()))\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error evaluating expression: {e!s}\") from Exception\n",
    "\n",
    "\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[basic_calculator], memory=UnconstrainedMemory()))\n",
    "result: BeeRunOutput = await agent.run(run_input=BeeRunInput(prompt=\"What is the square root of 36?\"))\n",
    "print(result.result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Tools\n",
    "\n",
    "Tools can also be imported from other libraries to extend the functionality of your agent. For example, you can integrate tools from libraries like LangChain to give your agent access to even more capabilities.\n",
    "\n",
    "Hereâ€™s an example showing how to integrate a tool from LangChain, written in long form (without using the @tool decorator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current president of the European Commission is Ursula von der Leyen.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from beeai_framework.agents.bee.agent import BeeAgent\n",
    "from beeai_framework.tools import Tool\n",
    "\n",
    "\n",
    "class LangChainWikipediaToolInput(BaseModel):\n",
    "    query: str = Field(description=\"The topic or question to search for on Wikipedia.\")\n",
    "\n",
    "\n",
    "class LangChainWikipediaTool(Tool):\n",
    "    \"\"\"Adapter class to integrate LangChain's Wikipedia tool with our framework\"\"\"\n",
    "\n",
    "    name = \"Wikipedia\"\n",
    "    description = \"Search factual and historical information from Wikipedia about given topics.\"\n",
    "    input_schema = LangChainWikipediaToolInput\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        wikipedia = WikipediaAPIWrapper()\n",
    "        self.wikipedia = WikipediaQueryRun(api_wrapper=wikipedia)\n",
    "\n",
    "    def _run(self, input: LangChainWikipediaToolInput, _: Any | None = None) -> None:\n",
    "        query = input.query\n",
    "        try:\n",
    "            result = self.wikipedia.run(query)\n",
    "            return StringToolOutput(result=result)\n",
    "        except Exception as e:\n",
    "            print(f\"Wikipedia search error: {e!s}\")\n",
    "            return f\"Error searching Wikipedia: {e!s}\"\n",
    "\n",
    "\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[LangChainWikipediaTool()], memory=UnconstrainedMemory()))\n",
    "result: BeeRunOutput = await agent.run(\n",
    "    run_input=BeeRunInput(prompt=\"Who is the current president of the European Commission?\")\n",
    ")\n",
    "print(result.result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example can be re-written in a shorter form by adding the @tool decorator, which simplifies the tool definition. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current president of the European Commission is Ursula von der Leyen.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun  # noqa: F811\n",
    "from langchain_community.utilities import WikipediaAPIWrapper  # noqa: F811\n",
    "\n",
    "from beeai_framework.agents.bee.agent import BeeAgent\n",
    "from beeai_framework.tools import Tool\n",
    "\n",
    "\n",
    "# defining a tool using the `tool` decorator\n",
    "# Note: the pydoc is important as it serves as the tool description to the agent\n",
    "@tool\n",
    "def langchain_wikipedia_tool(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Search factual and historical information, including biography, history, politics, geography, society, culture,\n",
    "    science, technology, people, animal species, mathematics, and other subjects.\n",
    "\n",
    "    Args:\n",
    "        expression: The topic or question to search for on Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "        The information found via searching Wikipedia.\n",
    "    \"\"\"\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    return StringToolOutput(wikipedia.run(expression))\n",
    "\n",
    "\n",
    "# using the tool in an agent\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[langchain_wikipedia_tool], memory=UnconstrainedMemory()))\n",
    "result: BeeRunOutput = await agent.run(\n",
    "    run_input=BeeRunInput(prompt=\"Who is the current president of the European Commission?\")\n",
    ")\n",
    "print(result.result.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
