{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeeAI ReAct agents\n",
    "\n",
    "The BeeAI ReAct agent is a pre-canned ReAct agent implementation that can be configured with tools and instructions. \n",
    "\n",
    "The ReAct pattern, short for Reasoning and Acting, is a framework used in AI models, particularly in language models, that separates the reasoning process from the action-taking process. \n",
    "It enhances the model's ability to handle complex queries by allowing it to reason about the problem, decide on an action, and then observe the result of that action to inform further reasoning and actions. \n",
    "\n",
    "The ReAct agent provides a convenient out of the box agent implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ReAct Agent\n",
    "\n",
    "To configure a ReAct agent you will need to define a ChatModel and construct a BeeAgent.\n",
    "\n",
    "In this example we do not provide any tools to the agent, it will attempt to provide an answer from its own memory.\n",
    "\n",
    "Try changing the text input in the call to `agent.run()` to experiment with getting different answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from beeai_framework.agents.bee.agent import BeeAgent\n",
    "from beeai_framework.agents.types import BeeInput, BeeRunInput, BeeRunOutput\n",
    "from beeai_framework.backend.chat import ChatModel\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# Construct ChatModel\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "\n",
    "# Construct Agent instance with the chat model\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[], memory=UnconstrainedMemory()))\n",
    "\n",
    "# Run the agent\n",
    "result: BeeRunOutput = await agent.run(run_input=BeeRunInput(prompt=\"What chemical elements make up a water molecule?\"))\n",
    "\n",
    "print(result.result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tools\n",
    "\n",
    "To move beyond chatting to an LLM, tools can be provided to the agent.  There are different ways to do this via built-in tools from the framework, importing tools from other libraries or writing your own custom tooling.\n",
    "\n",
    "## Built-in tools\n",
    "\n",
    "BeeAI comes with some built-in tools that are provided as part of the library.  These can easily be imported and added to the agent.\n",
    "\n",
    "Your agent is now capable of doing a little more than the underlying Large Language Model is able to do by itself as it can now rely on tools to fetch additional knowledge/context from elsewhere.\n",
    "\n",
    "In this example we give the agent a weather forecast lookup tool called OpenMeteoTool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beeai_framework.backend.chat import ChatModel\n",
    "from beeai_framework.tools.weather.openmeteo import OpenMeteoTool\n",
    "\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "\n",
    "# create an agent using the default LLM and add the OpenMeteoTool that is capable of fetching weather-based information\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[OpenMeteoTool()], memory=UnconstrainedMemory()))\n",
    "\n",
    "result: BeeRunOutput = await agent.run(run_input=BeeRunInput(prompt=\"What's the current weather in London?\"))\n",
    "\n",
    "print(result.result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tools\n",
    "\n",
    "Tools can be written from scratch and added to your agent.\n",
    "\n",
    "Use the `@tool` decorator on your tool function and pass it into the agent in the same way as built-int tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.parse import quote\n",
    "\n",
    "import requests\n",
    "\n",
    "from beeai_framework import BeeAgent, tool\n",
    "from beeai_framework.backend.chat import ChatModel\n",
    "from beeai_framework.tools.tool import StringToolOutput\n",
    "from beeai_framework.tools.weather.openmeteo import OpenMeteoTool\n",
    "\n",
    "\n",
    "# defining a tool using the `tool` decorator\n",
    "# Note: the pydoc is important as it serves as the tool description to the agent\n",
    "@tool\n",
    "def basic_calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    A calculator tool that performs mathematical operations.\n",
    "\n",
    "    Args:\n",
    "        expression: The mathematical expression to evaluate (e.g., \"2 + 3 * 4\").\n",
    "\n",
    "    Returns:\n",
    "        The result of the mathematical expression\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoded_expression = quote(expression)\n",
    "        math_url = f\"https://newton.vercel.app/api/v2/simplify/{encoded_expression}\"\n",
    "\n",
    "        response = requests.get(\n",
    "            math_url,\n",
    "            headers={\"Accept\": \"application/json\"},\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return StringToolOutput(json.dumps(response.json()))\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error evaluating expression: {e!s}\") from Exception\n",
    "\n",
    "\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[basic_calculator], memory=UnconstrainedMemory()))\n",
    "result: BeeRunOutput = await agent.run(run_input=BeeRunInput(prompt=\"What is the square root of 36?\"))\n",
    "print(result.result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Tools\n",
    "\n",
    "Tools can be imported from other libraries.\n",
    "\n",
    "The example below shows how to integrate a tool from LangChain.  It also demonstrates long form tool writing without the use of the `@tool` decorator (see below for how this can be re-written in short form using the `@tool` decorator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from beeai_framework import BeeAgent\n",
    "from beeai_framework.tools import Tool\n",
    "\n",
    "\n",
    "class LangChainWikipediaToolInput(BaseModel):\n",
    "    query: str = Field(description=\"The topic or question to search for on Wikipedia.\")\n",
    "\n",
    "\n",
    "class LangChainWikipediaTool(Tool):\n",
    "    \"\"\"Adapter class to integrate LangChain's Wikipedia tool with our framework\"\"\"\n",
    "\n",
    "    name = \"Wikipedia\"\n",
    "    description = \"Search factual and historical information from Wikipedia about given topics.\"\n",
    "    input_schema = LangChainWikipediaToolInput\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        wikipedia = WikipediaAPIWrapper()\n",
    "        self.wikipedia = WikipediaQueryRun(api_wrapper=wikipedia)\n",
    "\n",
    "    def _run(self, input: LangChainWikipediaToolInput, _: Any | None = None) -> None:\n",
    "        query = input.query\n",
    "        try:\n",
    "            result = self.wikipedia.run(query)\n",
    "            return StringToolOutput(result=result)\n",
    "        except Exception as e:\n",
    "            print(f\"Wikipedia search error: {e!s}\")\n",
    "            return f\"Error searching Wikipedia: {e!s}\"\n",
    "\n",
    "\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[LangChainWikipediaTool()], memory=UnconstrainedMemory()))\n",
    "result: BeeRunOutput = await agent.run(\n",
    "    run_input=BeeRunInput(prompt=\"Who is the current president of the European Commission?\")\n",
    ")\n",
    "print(result.result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example can be re-written in shorter form by adding the `@tool` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun  # noqa: F811\n",
    "from langchain_community.utilities import WikipediaAPIWrapper  # noqa: F811\n",
    "\n",
    "from beeai_framework import BeeAgent, tool\n",
    "\n",
    "\n",
    "# defining a tool using the `tool` decorator\n",
    "# Note: the pydoc is important as it serves as the tool description to the agent\n",
    "@tool\n",
    "def langchain_wikipedia_tool(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Search factual and historical information, including biography, history, politics, geography, society, culture,\n",
    "    science, technology, people, animal species, mathematics, and other subjects.\n",
    "\n",
    "    Args:\n",
    "        expression: The topic or question to search for on Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "        The information found via searching Wikipedia.\n",
    "    \"\"\"\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    return StringToolOutput(wikipedia.run(expression))\n",
    "\n",
    "\n",
    "# using the tool in an agent\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[langchain_wikipedia_tool], memory=UnconstrainedMemory()))\n",
    "result: BeeRunOutput = await agent.run(\n",
    "    run_input=BeeRunInput(prompt=\"Who is the current president of the European Commission?\")\n",
    ")\n",
    "print(result.result.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beeai-iRW9JlkS-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
