{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeeAI Workflows\n",
    "\n",
    "In the previous notebook, you learned the basics of the BeeAI framework, including PromptTemplates, Messages, Memory, Model Backends, and various forms of output generation (freeform and structured). In this notebook, we will focus on Workflows.\n",
    "\n",
    "Workflows enable you to combine what you've already learned to develop an AI agent. The agent's behavior is defined through workflow steps and the transitions between them. You can think of a Workflow as a graph that outlines the agent's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Workflows\n",
    "\n",
    "The main components of a BeeAI workflow are state, defined as a Pydantic model, and steps, which are Python functions.\n",
    "\n",
    "- State: Think of state as structured memory that the workflow can read from and write to during execution. It holds the data that flows through the workflow.\n",
    "- Steps: These are the functional components of the workflow, connecting together to perform the agent’s actions.\n",
    "\n",
    "The following simple workflow example highlights these key features:\n",
    "\n",
    "- The state definition includes a required message field.\n",
    "- The step (my_first_step) is defined as a function that takes the state instance as a parameter.\n",
    "- The state can be modified within a step, and changes to the state are preserved across steps and workflow executions.\n",
    "- The step function returns a string (Workflow.END), indicating the name of the next step (this is how step transitions are handled).\n",
    "- Workflow.END signifies the end of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running first step!\n",
      "State after workflow run: message='Hello World'\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "from beeai_framework.workflows.workflow import Workflow, WorkflowError\n",
    "\n",
    "\n",
    "# Define global state that is accessible to each step in the workflow graph\n",
    "# The message field is required when instantiating the state object\n",
    "class MessageState(BaseModel):\n",
    "    message: str\n",
    "\n",
    "\n",
    "# Each step in the workflow is defined as a python function\n",
    "async def my_first_step(state: MessageState) -> None:\n",
    "    state.message += \" World\"  # Modify the state\n",
    "    print(\"Running first step!\")\n",
    "    return Workflow.END\n",
    "\n",
    "\n",
    "try:\n",
    "    # Define the structure of the workflow graph\n",
    "    basic_workflow = Workflow(schema=MessageState, name=\"MyWorkflow\")\n",
    "\n",
    "    # Add a step, each step has a name and a function that implements the step\n",
    "    basic_workflow.add_step(\"my_first_step\", my_first_step)\n",
    "\n",
    "    # Execute the workflow\n",
    "    basic_response = await basic_workflow.run(MessageState(message=\"Hello\"))\n",
    "\n",
    "    print(\"State after workflow run:\", basic_response.state)\n",
    "\n",
    "except WorkflowError:\n",
    "    traceback.print_exc()\n",
    "except ValidationError:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Multi-Step Workflow with Tools\n",
    "\n",
    "Now that you understand the basic components of a Workflow, let’s explore the power of BeeAI Workflows by building a simple web search agent.\n",
    "\n",
    "This agent creates a search query based on an input question, runs the query to retrieve search results, and then generates an answer to the question based on the results.\n",
    "\n",
    "Let’s begin by importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SearxSearchWrapper\n",
    "from pydantic import Field\n",
    "\n",
    "from beeai_framework.backend.chat import ChatModel, ChatModelOutput, ChatModelStructureOutput\n",
    "from beeai_framework.backend.message import UserMessage\n",
    "from beeai_framework.utils.templates import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define our workflow State.\n",
    "\n",
    "In this case, the question field is required when instantiating the State. The other fields, search_results and answer, are optional during construction (defaulting to None), but they will be populated by the workflow steps as the execution progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow State\n",
    "class SearchAgentState(BaseModel):\n",
    "    question: str\n",
    "    search_results: str | None = None\n",
    "    answer: str | None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the ChatModel instance that will handle interaction with our LLM. For this example, we'll use IBM Granite 3.1 8B via Ollama. This model will be used to process the search query and generate answers based on the retrieved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatModel to interface with granite3.1-dense:8b on a local ollama\n",
    "model = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a web search agent, we need a way to run web searches. For that, we'll use the SearxSearchWrapper from the Langchain community tools project.\n",
    "\n",
    "To use the SearxSearchWrapper, you'll need to set up a local SearXNG service.\n",
    "\n",
    "Follow the instructions in [searXNG.md](searXNG.md) to configure your local SearXNG instance before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web search tool\n",
    "search_tool = SearxSearchWrapper(searx_host=\"http://127.0.0.1:8888\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workflow, we make extensive use of PromptTemplates and structured outputs.\n",
    "\n",
    "Here, we define the various templates, input schemas, and structured output schemas that are essential for implementing the agent. These templates will allow us to generate the search query and structure the results in a way that the agent can process effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate Input Schemas\n",
    "class QuestionInput(BaseModel):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class SearchRAGInput(BaseModel):\n",
    "    question: str\n",
    "    search_results: str\n",
    "\n",
    "\n",
    "# Prompt Templates\n",
    "search_query_template = PromptTemplate(\n",
    "    schema=QuestionInput,\n",
    "    template=\"\"\"Convert the following question into a concise, effective web search query using keywords and operators for accuracy.\n",
    "Question: {{question}}\"\"\",\n",
    ")\n",
    "\n",
    "search_rag_template = PromptTemplate(\n",
    "    schema=SearchRAGInput,\n",
    "    template=\"\"\"Search results:\n",
    "{{search_results}}\n",
    "\n",
    "Question: {{question}}\n",
    "Provide a concise answer based on the search results provided. If the results are irrelevant or insufficient, say 'I don't know.' Avoid phrases such as 'According to the results...'.\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "# Structured output Schemas\n",
    "class WebSearchQuery(BaseModel):\n",
    "    query: str = Field(description=\"The web search query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the first step of the workflow, named web_search.\n",
    "\n",
    "In this step:\n",
    "\n",
    "- The LLM is prompted to generate an effective search query using the search_query_template.\n",
    "- The generated search query is then used to run a web search via the search tool (SearxSearchWrapper).\n",
    "- The search results are stored in the search_results field of the workflow state.\n",
    "- Finally, the step returns generate_answer, passing control to the next step, named generate_answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def web_search(state: SearchAgentState) -> str:\n",
    "    print(\"Step: \", \"web_search\")\n",
    "    # Generate a search query\n",
    "    prompt = search_query_template.render(QuestionInput(question=state.question))\n",
    "    response: ChatModelStructureOutput = await model.create_structure(\n",
    "        {\n",
    "            \"schema\": WebSearchQuery,\n",
    "            \"messages\": [UserMessage(prompt)],\n",
    "        }\n",
    "    )\n",
    "    # Run search and store results in state\n",
    "    state.search_results = search_tool.run(response.object[\"query\"])\n",
    "    return \"generate_answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the workflow is generate_answer.\n",
    "\n",
    "This step:\n",
    "\n",
    "- Takes the question and search_results from the workflow state.\n",
    "- Uses the search_rag_template to generate an answer based on the provided data.\n",
    "- The generated answer is stored in the workflow state.\n",
    "- Finally, the workflow ends by returning Workflow.END, signaling the completion of the agent’s task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_answer(state: SearchAgentState) -> str:\n",
    "    print(\"Step: \", \"generate_answer\")\n",
    "    # Generate answer based on question and search results from previous step.\n",
    "    prompt = search_rag_template.render(\n",
    "        SearchRAGInput(question=state.question, search_results=state.search_results or \"No results available.\")\n",
    "    )\n",
    "    output: ChatModelOutput = await model.create({\"messages\": [UserMessage(prompt)]})\n",
    "\n",
    "    # Store answer in state\n",
    "    state.answer = output.get_text_content()\n",
    "    return Workflow.END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the overall workflow and add the steps we developed earlier. This combines everything into a cohesive agent that can perform web searches and generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  web_search\n",
      "Step:  generate_answer\n",
      "*****\n",
      "Question:  What is the term for a baby hedgehog?\n",
      "Answer:  The term for a baby hedgehog is hoglet.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Define the structure of the workflow graph\n",
    "    search_agent_workflow = Workflow(schema=SearchAgentState, name=\"WebSearchAgent\")\n",
    "    search_agent_workflow.add_step(\"web_search\", web_search)\n",
    "    search_agent_workflow.add_step(\"generate_answer\", generate_answer)\n",
    "\n",
    "    # Execute the workflow\n",
    "    search_response = await search_agent_workflow.run(\n",
    "        SearchAgentState(question=\"What is the term for a baby hedgehog?\")\n",
    "    )\n",
    "\n",
    "    print(\"*****\")\n",
    "    print(\"Question: \", search_response.state.question)\n",
    "    print(\"Answer: \", search_response.state.answer)\n",
    "\n",
    "except WorkflowError:\n",
    "    traceback.print_exc()\n",
    "except ValidationError:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Memory to a Workflow Agent\n",
    "\n",
    "The web search agent from the previous example can answer questions, but it cannot engage in a conversation because it doesn't maintain message history.\n",
    "\n",
    "In the next example, we'll show you how to add memory to your agent, allowing it to interactively chat while keeping track of the conversation history. This will enable the agent to remember previous interactions and provide more context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ('exit' to cancel):  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  Hello! How can I assist you today? If you have any questions or need information on a specific topic, feel free to ask. I'm here to help. 😊\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ('exit' to cancel):  what is the name of a baby crow?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  A baby crow is commonly referred to as a \"squab\" or a \"chick.\" They are part of a group known as corvid chicks, which also includes baby ravens and magpies. Corvus is the scientific genus that includes crows, ravens, rooks, jackdaws, and other related birds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ('exit' to cancel):  exit\n"
     ]
    }
   ],
   "source": [
    "# Workflow State\n",
    "from pydantic import InstanceOf\n",
    "\n",
    "from beeai_framework.backend.message import AssistantMessage, SystemMessage\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "\n",
    "\n",
    "class ChatState(BaseModel):\n",
    "    memory: InstanceOf[UnconstrainedMemory]\n",
    "    output: str | None = None\n",
    "\n",
    "\n",
    "async def chat(state: ChatState) -> str:\n",
    "    output: ChatModelOutput = await model.create({\"messages\": state.memory.messages})\n",
    "    state.output = output.get_text_content()\n",
    "    return Workflow.END\n",
    "\n",
    "\n",
    "memory = UnconstrainedMemory()\n",
    "await memory.add(SystemMessage(content=\"You are a helpful and friendly AI assistant.\"))\n",
    "\n",
    "try:\n",
    "    # Define the structure of the workflow graph\n",
    "    chat_workflow = Workflow(ChatState)\n",
    "    chat_workflow.add_step(\"chat\", chat)\n",
    "    chat_workflow.add_step(\"generate_answer\", generate_answer)\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"User (type 'exit' to stop): \")\n",
    "        if user_input == \"exit\":\n",
    "            break\n",
    "        # Add user message to memory\n",
    "        await memory.add(UserMessage(content=user_input))\n",
    "        # Run workflow with memory\n",
    "        response = await chat_workflow.run(ChatState(memory=memory))\n",
    "        # Add assistant response to memory\n",
    "        await memory.add(AssistantMessage(content=response.state.output))\n",
    "        print(\"Assistant: \", response.state.output)\n",
    "\n",
    "except WorkflowError:\n",
    "    traceback.print_exc()\n",
    "except ValidationError:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct Agents\n",
    "\n",
    "You are now familiar with Workflow based agents, next you can explore pre-canned ReAct agents. Move on to [agents.ipynb](agents.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
