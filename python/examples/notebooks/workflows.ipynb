{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeeAI Workflows\n",
    "\n",
    "In the previous notebook you learned the basics of the BeeAI framework such as PromptTemplates, Messages, Memory, Model Backends and various forms of output generation (freeform & structured). In this notebook we will focus on Workflows.\n",
    "\n",
    "Workflows allow you to combine what you have already learned to develop an AI agent. The behavior of the agent is defined via workflow steps, and transitions between those steps. You can think of the Workflow as a graph that describes the behavior of an agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Workflows\n",
    "\n",
    "The main components of a BeeAI workflow is state, defined as a pydantic model, and steps, which are defined using python functions.\n",
    "\n",
    "You can think of State as structured memory that the workflow can read and write during execution.\n",
    "\n",
    "Steps are the the functional components of the Workflow that connect together to perform the actions of the agent.\n",
    "\n",
    "The following simple workflow example exhibits the following key features: \n",
    "\n",
    "- The state definition contains a required message field.\n",
    "- The step (my_first_step) is defined as a function parameterized with the state instance.\n",
    "- The state can be modified in a step and state changes are persisted between steps and workflow executions.\n",
    "- The step function returns a string `Workflow.END` which indicates the name of the next step (this is how step transitions are implemented).\n",
    "- `Workflow.END` indicates the end of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import warnings\n",
    "\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "from beeai_framework.workflows.workflow import Workflow, WorkflowError\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "\n",
    "# Define global state that is accessible to each step in the workflow graph\n",
    "# The message field is required when instantiating the state object\n",
    "class MessageState(BaseModel):\n",
    "    message: str\n",
    "\n",
    "\n",
    "# Each step in the workflow is defined as a python function\n",
    "async def my_first_step(state: MessageState) -> None:\n",
    "    state.message += \" World\"  # Modify the state\n",
    "    print(\"Running first step!\")\n",
    "    return Workflow.END\n",
    "\n",
    "\n",
    "try:\n",
    "    # Define the structure of the workflow graph\n",
    "    basic_workflow = Workflow(schema=MessageState, name=\"MyWorkflow\")\n",
    "\n",
    "    # Add a step, each step has a name and a function that implements the step\n",
    "    basic_workflow.add_step(\"my_first_step\", my_first_step)\n",
    "\n",
    "    # Execute the workflow\n",
    "    basic_response = await basic_workflow.run(MessageState(message=\"Hello\"))\n",
    "\n",
    "    print(\"State after workflow:\", basic_response.state.message)\n",
    "\n",
    "except WorkflowError:\n",
    "    traceback.print_exc()\n",
    "except ValidationError:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Multi step workflow with tools\n",
    "\n",
    "You now know the basic components of a Workflow. To explore the power of BeeAI Workflows we will now walk through the implementation of a simple web search agent built as a Workflow.\n",
    "\n",
    "This agent devises a search query based on an input question, runs the query to get search results, and then generates an answer to the question based on the retrieved search results.\n",
    "\n",
    "Lets start with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SearxSearchWrapper\n",
    "from pydantic import Field\n",
    "\n",
    "from beeai_framework.backend.chat import ChatModel, ChatModelOutput, ChatModelStructureOutput\n",
    "from beeai_framework.backend.message import UserMessage\n",
    "from beeai_framework.utils.templates import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can define our workflow State.\n",
    "\n",
    "In this case we have a `question` which is a required field when instantiating the State. The other fields `search_results` and `answer` are optional during construction (defaulting to None) but will be populated by the workflow steps during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow State\n",
    "class SearchAgentState(BaseModel):\n",
    "    question: str\n",
    "    search_results: str | None = None\n",
    "    answer: str | None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the ChatModel instance that we use for interaction with our LLM. We will use IBM Granite 3.1 8B via ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatModel to interface with granite3.1-dense:8b on a local ollama\n",
    "model = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a web search agent, so we need a way to run web searches. We will use the `SearxSearchWrapper` from the langchain community tools project.\n",
    "\n",
    "To use the `SearxSearchWrapper` you will need to setup a local SearXNG service. \n",
    "\n",
    "Follow the instructions at [searXNG.md](searXNG.md) to configure a local searXNG instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web search tool\n",
    "search_tool = SearxSearchWrapper(searx_host=\"http://127.0.0.1:8888\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the workflow we make extensive use of prompt templates and structured outputs.\n",
    "\n",
    "Here we define the various templates, input schemas and structured output schemas that we will need to implement the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate Input Schemas\n",
    "class QuestionInput(BaseModel):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class SearchRAGInput(BaseModel):\n",
    "    question: str\n",
    "    search_results: str\n",
    "\n",
    "\n",
    "# Prompt Templates\n",
    "search_query_template = PromptTemplate(\n",
    "    schema=QuestionInput,\n",
    "    template=\"\"\"Convert the following question into a concise, effective web search query using keywords and operators for accuracy.\n",
    "Question: {{question}}\"\"\",\n",
    ")\n",
    "\n",
    "search_rag_template = PromptTemplate(\n",
    "    schema=SearchRAGInput,\n",
    "    template=\"\"\"Search results:\n",
    "{{search_results}}\n",
    "\n",
    "Question: {{question}}\n",
    "Provide a concise answer based on the search results provided. If the results are irrelevant or insufficient, say 'I don't know.' Avoid phrases such as 'According to the results...'.\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "# Structured output Schemas\n",
    "class WebSearchQuery(BaseModel):\n",
    "    query: str = Field(description=\"The web search query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the first step of the workflow named `web_search`. \n",
    "\n",
    "This step prompts the llm to generate an effective search query using the search_query_template.\n",
    "\n",
    "The search query is then used to run a web search using the search tool. The search results are stored in the `search_results` field in the workflow state.\n",
    "\n",
    "The step then returns `generate_answer` which passes control to the step names `generate_answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def web_search(state: SearchAgentState) -> str:\n",
    "    print(\"Step: \", \"web_search\")\n",
    "    # Generate a search query\n",
    "    prompt = search_query_template.render(QuestionInput(question=state.question))\n",
    "    response: ChatModelStructureOutput = await model.create_structure(\n",
    "        {\n",
    "            \"schema\": WebSearchQuery,\n",
    "            \"messages\": [UserMessage(prompt)],\n",
    "        }\n",
    "    )\n",
    "    # Run search and store results in state\n",
    "    state.search_results = search_tool.run(response.object[\"query\"])\n",
    "    return \"generate_answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the Workflow is `generate_answer`, this steps takes the `question` and the `search_results` from the workflow state and uses the search_rag_template to generate an answer.\n",
    "\n",
    "The answer is stored in the state and the workflow is ended by returning `Workflow.END`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_answer(state: SearchAgentState) -> str:\n",
    "    print(\"Step: \", \"generate_answer\")\n",
    "    # Generate answer based on question and search results from previous step.\n",
    "    prompt = search_rag_template.render(\n",
    "        SearchRAGInput(question=state.question, search_results=state.search_results or \"No results available.\")\n",
    "    )\n",
    "    output: ChatModelOutput = await model.create({\"messages\": [UserMessage(prompt)]})\n",
    "\n",
    "    # Store answer in state\n",
    "    state.answer = output.get_text_content()\n",
    "    return Workflow.END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FInally we define the overall workflow and add the steps we developed earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Define the structure of the workflow graph\n",
    "    search_agent_workflow = Workflow(schema=SearchAgentState, name=\"WebSearchAgent\")\n",
    "    search_agent_workflow.add_step(\"web_search\", web_search)\n",
    "    search_agent_workflow.add_step(\"generate_answer\", generate_answer)\n",
    "\n",
    "    # Execute the workflow\n",
    "    search_response = await search_agent_workflow.run(\n",
    "        SearchAgentState(question=\"What is the term for a baby hedgehog?\")\n",
    "    )\n",
    "\n",
    "    print(\"*****\")\n",
    "    print(\"Question: \", search_response.state.question)\n",
    "    print(\"Answer: \", search_response.state.answer)\n",
    "\n",
    "except WorkflowError:\n",
    "    traceback.print_exc()\n",
    "except ValidationError:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Memory to a Workflow Agent\n",
    "\n",
    "The web search agent from the previous example can answer questions but is unable to converse because it does not maintain message history. \n",
    "\n",
    "In the next example we show you how to add memory to your agent, so you can chat interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow State\n",
    "from pydantic import InstanceOf\n",
    "\n",
    "from beeai_framework.backend.message import AssistantMessage, SystemMessage\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "\n",
    "\n",
    "class ChatState(BaseModel):\n",
    "    memory: InstanceOf[UnconstrainedMemory]\n",
    "    output: str | None = None\n",
    "\n",
    "\n",
    "async def chat(state: ChatState) -> str:\n",
    "    output: ChatModelOutput = await model.create({\"messages\": state.memory.messages})\n",
    "    state.output = output.get_text_content()\n",
    "    return Workflow.END\n",
    "\n",
    "\n",
    "memory = UnconstrainedMemory()\n",
    "await memory.add(SystemMessage(content=\"You are a helpful and friendly AI assistant.\"))\n",
    "\n",
    "try:\n",
    "    # Define the structure of the workflow graph\n",
    "    chat_workflow = Workflow(ChatState)\n",
    "    chat_workflow.add_step(\"chat\", chat)\n",
    "    chat_workflow.add_step(\"generate_answer\", generate_answer)\n",
    "\n",
    "    # Add user message to memory\n",
    "    await memory.add(UserMessage(content=input(\"User: \")))\n",
    "    # Run workflow with memory\n",
    "    response = await chat_workflow.run(ChatState(memory=memory))\n",
    "    # Add assistant response to memory\n",
    "    await memory.add(AssistantMessage(content=response.state.output))\n",
    "\n",
    "    print(\"\\n\".join(f\"{m.role}: {m.text}\" for m in memory.messages))\n",
    "\n",
    "except WorkflowError:\n",
    "    traceback.print_exc()\n",
    "except ValidationError:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct Agents\n",
    "\n",
    "You are now familiar with Workflow based agents, next you can explore pre-canned ReAct agents. Move on to [agents.ipynb](agents.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beeai-iRW9JlkS-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
