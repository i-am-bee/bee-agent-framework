{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeeAI Framework Basics\n",
    "\n",
    "These examples show some of the basic usage patterns of BeeAI in Python.  They gradually build in complexity to give a rounded overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "One of the most basic constructs provided by the BeeAI framework is the `PromptTemplate`. Using a PromptTemplate you can incorporate data into a prompt before sending it a language model.\n",
    "Prompt templates are based on the mustache templating language.\n",
    "\n",
    "The following example shows you how to create a RAG (Retrieval Augmented Generation) template and apply the template to your data to generate a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from beeai_framework.utils.templates import PromptTemplate\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "\n",
    "# The input schema model: Defines the structure of the input data that can passed to the template\n",
    "class RAGTemplateInput(BaseModel):\n",
    "    question: str\n",
    "    context: str\n",
    "\n",
    "\n",
    "# Define the template\n",
    "rag_template: PromptTemplate = PromptTemplate(\n",
    "    schema=RAGTemplateInput,\n",
    "    template=\"\"\"\n",
    "Context: {{context}}\n",
    "Question: {{question}}\n",
    "\n",
    "Provide a concise answer based on the context. Avoid statements such as 'Based on the context' or 'According to the context' etc. \"\"\",\n",
    ")\n",
    "\n",
    "# Render the template using an instance of the input model\n",
    "prompt = rag_template.render(\n",
    "    RAGTemplateInput(\n",
    "        question=\"What is the capital of France?\",\n",
    "        context=\"France is a country in Europe. Its capital city is Paris, known for its culture and history.\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex templates\n",
    "\n",
    "That was a simple template but the `PromptTemplate` class can also be used to render more complex objects and include conditional logic.\n",
    "\n",
    "The next example is a template that includes a question and a set of complex search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from beeai_framework.utils.templates import PromptTemplate\n",
    "\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class SearchTemplateInput(BaseModel):\n",
    "    question: str\n",
    "    results: list[SearchResult]\n",
    "\n",
    "\n",
    "# Define the template\n",
    "search_template: PromptTemplate = PromptTemplate(\n",
    "    schema=SearchTemplateInput,\n",
    "    template=\"\"\"\n",
    "Search results:\n",
    "{{#results.0}}\n",
    "{{#results}}\n",
    "Title: {{title}}\n",
    "Url: {{url}}\n",
    "Content: {{content}}\n",
    "{{/results}}\n",
    "{{/results.0}}\n",
    "\n",
    "Question: {{question}}\n",
    "Provide a concise answer based on the search results provided.\"\"\",\n",
    ")\n",
    "\n",
    "# Render the template using an instance of the input model\n",
    "prompt = search_template.render(\n",
    "    SearchTemplateInput(\n",
    "        question=\"What is the capital of France?\",\n",
    "        results=[\n",
    "            SearchResult(\n",
    "                title=\"France\",\n",
    "                url=\"https://en.wikipedia.org/wiki/France\",\n",
    "                content=\"France is a country in Europe. Its capital city is Paris, known for its culture and history.\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ChatModel\n",
    "\n",
    "Once you have your PromptTemplate you can start to think about prompting a model. BeeAI supports a variety of LLM's that you can use via the `ChatModel` interface. \n",
    "\n",
    "In this section we will use the `IBM Granite 3.1 8B` language model via the Ollama provider.\n",
    "\n",
    "[How to run Granite 3.1 using Ollama](https://www.ibm.com/granite/docs/run/granite-on-mac/granite/).\n",
    "\n",
    "Before creating a ChatModel we need to briefly cover Messages. The `ChatModel` interface operates using messages. Using messages you can represent a chat between the user and the assistant (the LLM) which is a convenient interaction method. Lets start by creating a `UserMessage` to say hello and ask a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beeai_framework.backend.message import UserMessage\n",
    "\n",
    "# Create a user message to start a chat with the model\n",
    "user_message = UserMessage(content=\"Hello! Can you tell me what is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a `ChatModel` and send this message to Granite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beeai_framework.backend.chat import ChatModel, ChatModelInput, ChatModelOutput\n",
    "\n",
    "# Create a ChatModel to interface with granite3.1-dense:8b on a local ollama\n",
    "model = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "\n",
    "output: ChatModelOutput = await model.create(ChatModelInput(messages=[user_message]))\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory \n",
    "The model has provided a response! We can now start to build up a `Memory`. Memory is just a convenient way of storing a set of messages that can be considered as the history of the dialog between the user and the llm.\n",
    "\n",
    "In this next example we will construct a memory from our existing messages and add a new user message. Notice that the new message can implicitly refer to content from prior messages. Internally the `ChatModel` formats all the messages and sends them to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beeai_framework.backend.message import AssistantMessage\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "\n",
    "memory = UnconstrainedMemory()\n",
    "\n",
    "await memory.add_many(\n",
    "    [\n",
    "        user_message,\n",
    "        AssistantMessage(content=output.get_text_content()),\n",
    "        UserMessage(content=\"If you had to recommend one thing to do there, what would it be?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output: ChatModelOutput = await model.create(ChatModelInput(messages=memory.messages))\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Templates and Messages\n",
    "\n",
    "If you would like to use a `PromptTemplate` from earlier with the Granite ChatModel, you can render the template and then put the content into a Message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some context that the model will use to provide an answer. Source wikipedia: https://en.wikipedia.org/wiki/Ireland\n",
    "context = \"\"\"The geography of Ireland comprises relatively low-lying mountains surrounding a central plain, with several navigable rivers extending inland.\n",
    "Its lush vegetation is a product of its mild but changeable climate which is free of extremes in temperature.\n",
    "Much of Ireland was woodland until the end of the Middle Ages. Today, woodland makes up about 10% of the island,\n",
    "compared with a European average of over 33%, with most of it being non-native conifer plantations.\n",
    "The Irish climate is influenced by the Atlantic Ocean and thus very moderate, and winters are milder than expected for such a northerly area,\n",
    "although summers are cooler than those in continental Europe. Rainfall and cloud cover are abundant.\n",
    "\"\"\"\n",
    "\n",
    "# Lets reuse our RAG template from earlier!\n",
    "prompt = rag_template.render(RAGTemplateInput(question=\"How much of Ireland is forested?\", context=context))\n",
    "\n",
    "output: ChatModelOutput = await model.create(ChatModelInput(messages=[UserMessage(content=prompt)]))\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs\n",
    "\n",
    "Sometimes (often!) you will want llm output in a specific format. This will allow you to interface the llm with your code in a reliable manner i.e. if you want the llm to produce the input to a function or tool. To achieve this you can use structured output.\n",
    "\n",
    "In the example below I want Granite to generate a character using a very specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class CharacterSchema(BaseModel):\n",
    "    name: str = Field(description=\"The name of the character.\")\n",
    "    occupation: str = Field(description=\"The occupation of the character.\")\n",
    "    species: Literal[\"Human\", \"Insectoid\", \"Void-Serpent\", \"Synth\", \"Ethereal\", \"Liquid-Metal\"] = Field(\n",
    "        description=\"The race of the character.\"\n",
    "    )\n",
    "    back_story: str = Field(description=\"Brief backstory of this character.\")\n",
    "\n",
    "\n",
    "user_message = UserMessage(\n",
    "    \"Crete a fantasy sci-fi character for my new game. This character will be the main protagonist.\"\n",
    ")\n",
    "response = await model.create_structure(\n",
    "    {\n",
    "        \"schema\": CharacterSchema,\n",
    "        \"messages\": [user_message],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts\n",
    "\n",
    "The system prompt or SystemMessage is a special message type that can influence the general behavior of an LLM. If you would like to influence an LLM in a general manner you can include a SystemMessage.\n",
    "\n",
    "In the example below we add a system message that influences the LLM to speak like a pirate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beeai_framework.backend.message import SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"You are pirate. You always respond using pirate slang.\")\n",
    "user_message = UserMessage(content=\"What is a baby hedgehog called?\")\n",
    "output: ChatModelOutput = await model.create(ChatModelInput(messages=[system_message, user_message]))\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Agent\n",
    "\n",
    "You are now ready to build you first agent. Move on to [workflows.ipynb](workflows.ipynb).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beeai-iRW9JlkS-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
