{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeeAI Framework Basics\n",
    "\n",
    "These examples demonstrate fundamental usage patterns of BeeAI in Python. They progressively increase in complexity, providing a well-rounded overview of the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "One of the core constructs in the BeeAI framework is the PromptTemplate. It allows you to dynamically insert data into a prompt before sending it to a language model. BeeAI uses the Mustache templating language for prompt formatting.\n",
    "\n",
    "The following example demonstrates how to create a Retrieval-Augmented Generation (RAG) template and apply it to your data to generate a structured prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: France is a country in Europe. Its capital city is Paris, known for its culture and history.\n",
      "Question: What is the capital of France?\n",
      "\n",
      "Provide a concise answer based on the context. Avoid statements such as 'Based on the context' or 'According to the context' etc. \n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from beeai_framework.utils.templates import PromptTemplate\n",
    "\n",
    "\n",
    "# Defines the structure of the input data that can passed to the template i.e. the input schema\n",
    "class RAGTemplateInput(BaseModel):\n",
    "    question: str\n",
    "    context: str\n",
    "\n",
    "\n",
    "# Define the prompt template\n",
    "rag_template: PromptTemplate = PromptTemplate(\n",
    "    schema=RAGTemplateInput,\n",
    "    template=\"\"\"\n",
    "Context: {{context}}\n",
    "Question: {{question}}\n",
    "\n",
    "Provide a concise answer based on the context. Avoid statements such as 'Based on the context' or 'According to the context' etc. \"\"\",\n",
    ")\n",
    "\n",
    "# Render the template using an instance of the input model\n",
    "prompt = rag_template.render(\n",
    "    RAGTemplateInput(\n",
    "        question=\"What is the capital of France?\",\n",
    "        context=\"France is a country in Europe. Its capital city is Paris, known for its culture and history.\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the rendered prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex templates\n",
    "\n",
    "The previous example demonstrated a simple template, but the PromptTemplate class can also handle more complex structures and incorporate conditional logic.\n",
    "\n",
    "The following example showcases a template that includes a question along with a set of detailed search results represented as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search results:\n",
      "Title: France\n",
      "Url: https://en.wikipedia.org/wiki/France\n",
      "Content: France is a country in Europe. Its capital city is Paris, known for its culture and history.\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Provide a concise answer based on the search results provided.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from beeai_framework.utils.templates import PromptTemplate\n",
    "\n",
    "\n",
    "# Individual search result\n",
    "class SearchResult(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "# Input specificiation\n",
    "class SearchTemplateInput(BaseModel):\n",
    "    question: str\n",
    "    results: list[SearchResult]\n",
    "\n",
    "\n",
    "# Define the template, in this instance the template will iterate over the results\n",
    "search_template: PromptTemplate = PromptTemplate(\n",
    "    schema=SearchTemplateInput,\n",
    "    template=\"\"\"\n",
    "Search results:\n",
    "{{#results.0}}\n",
    "{{#results}}\n",
    "Title: {{title}}\n",
    "Url: {{url}}\n",
    "Content: {{content}}\n",
    "{{/results}}\n",
    "{{/results.0}}\n",
    "\n",
    "Question: {{question}}\n",
    "Provide a concise answer based on the search results provided.\"\"\",\n",
    ")\n",
    "\n",
    "# Render the template using an instance of the input model\n",
    "prompt = search_template.render(\n",
    "    SearchTemplateInput(\n",
    "        question=\"What is the capital of France?\",\n",
    "        results=[\n",
    "            SearchResult(\n",
    "                title=\"France\",\n",
    "                url=\"https://en.wikipedia.org/wiki/France\",\n",
    "                content=\"France is a country in Europe. Its capital city is Paris, known for its culture and history.\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the rendered prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ChatModel\n",
    "\n",
    "Once you have a PromptTemplate and can easily render prompts, you’re ready to start interacting with a model. BeeAI supports a variety of LLMs through the ChatModel interface.\n",
    "\n",
    "In this section, we will use the IBM `Granite 3.1 8B` language model via the Ollama provider.\n",
    "\n",
    "If you haven't set up Ollama yet, follow the [guide on running Granite 3.1 using Ollama](https://www.ibm.com/granite/docs/run/granite-on-mac/granite/).\n",
    "\n",
    "⚡ If you would prefer to use watsonx then you should check out [watsonx.ipynb](watsonx.ipynb) and edit the following examples to use a watsonx ChatModel provider. ⚡\n",
    "\n",
    "Before creating a ChatModel, we need to briefly discuss Messages. The ChatModel operates using message-based interactions, allowing you to structure conversations between the user and the assistant (LLM) naturally.\n",
    "\n",
    "Let’s start by creating a UserMessage to greet the assistant and ask a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beeai_framework.backend.message import UserMessage\n",
    "\n",
    "# Create a user message to start a chat with the model\n",
    "user_message = UserMessage(content=\"Hello! Can you tell me what is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a ChatModel and send this message to Granite for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. It's known for its iconic landmarks like the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. Paris is also famous for its art, culture, cuisine, and fashion.\n"
     ]
    }
   ],
   "source": [
    "from beeai_framework.backend.chat import ChatModel, ChatModelInput, ChatModelOutput\n",
    "\n",
    "# Create a ChatModel to interface with granite3.1-dense:8b on a local ollama\n",
    "model = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "\n",
    "output: ChatModelOutput = await model.create(ChatModelInput(messages=[user_message]))\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory \n",
    "The model has provided a response! We can now start to build up a `Memory`. Memory is just a convenient way of storing a set of messages that can be considered as the history of the dialog between the user and the llm.\n",
    "\n",
    "In this next example we will construct a memory from our existing messages and add a new user message. Notice that the new message can implicitly refer to content from prior messages. Internally the `ChatModel` formats all the messages and sends them to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting the Eiffel Tower is a must-do activity in Paris. You can either admire it from afar at the Champ de Mars or take an elevator ride up for breathtaking views of the city. Don't forget to enjoy a picnic in the nearby Jardin du Trocadéro or have dinner at one of the tower's restaurants for a truly memorable experience.\n"
     ]
    }
   ],
   "source": [
    "from beeai_framework.backend.message import AssistantMessage\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "\n",
    "memory = UnconstrainedMemory()\n",
    "\n",
    "await memory.add_many(\n",
    "    [\n",
    "        user_message,\n",
    "        AssistantMessage(content=output.get_text_content()),\n",
    "        UserMessage(content=\"If you had to recommend one thing to do there, what would it be?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output: ChatModelOutput = await model.create(ChatModelInput(messages=memory.messages))\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Templates and Messages\n",
    "\n",
    "To use a PromptTemplate with the Granite ChatModel, you can render the template and then place the resulting content into a Message. This allows you to dynamically generate prompts and pass them along as part of the conversation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently, around 10% of Ireland's land area is forested. Most of this woodland consists of non-native conifer plantations. The native woodland cover was significantly higher in the past but has diminished over time.\n"
     ]
    }
   ],
   "source": [
    "# Some context that the model will use to provide an answer. Source wikipedia: https://en.wikipedia.org/wiki/Ireland\n",
    "context = \"\"\"The geography of Ireland comprises relatively low-lying mountains surrounding a central plain, with several navigable rivers extending inland.\n",
    "Its lush vegetation is a product of its mild but changeable climate which is free of extremes in temperature.\n",
    "Much of Ireland was woodland until the end of the Middle Ages. Today, woodland makes up about 10% of the island,\n",
    "compared with a European average of over 33%, with most of it being non-native conifer plantations.\n",
    "The Irish climate is influenced by the Atlantic Ocean and thus very moderate, and winters are milder than expected for such a northerly area,\n",
    "although summers are cooler than those in continental Europe. Rainfall and cloud cover are abundant.\n",
    "\"\"\"\n",
    "\n",
    "# Lets reuse our RAG template from earlier!\n",
    "prompt = rag_template.render(RAGTemplateInput(question=\"How much of Ireland is forested?\", context=context))\n",
    "\n",
    "output: ChatModelOutput = await model.create(ChatModelInput(messages=[UserMessage(content=prompt)]))\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs\n",
    "\n",
    "Often, you'll want the LLM to produce output in a specific format. This ensures reliable interaction between the LLM and your code—such as when you need the LLM to generate input for a function or tool. To achieve this, you can use structured output.\n",
    "\n",
    "In the example below, we will prompt Granite to generate a character using a very specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Aelius Solara', 'occupation': 'Stellar Navigator & Arcane Cartographer', 'species': 'Liquid-Metal', 'back_story': \"Born in the heart of a dying star, Aelius Solara is a sentient liquid metal entity. Abandoned as a nebulous being, he was discovered and nurtured by an ancient race of cosmic explorers known as the 'Celestial Cartographers'. These beings taught him to manipulate his form, granting him the ability to navigate the vastness of space and traverse dimensions. He became a master in arcane cartography, capable of interpreting celestial maps that reveal hidden pathways across galaxies and parallel universes. His mission: to find 'The Nexus', an elusive cosmic node rumored to hold the key to staving off the impending doom of his own universe's star. A solitary figure, he carries an otherworldly charm, with a voice like swirling stars and eyes that shimmer with the light of distant galaxies.\"}\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "# The output structure definition, note the field descriptions that can help the LLM to understand the intention of the field.\n",
    "class CharacterSchema(BaseModel):\n",
    "    name: str = Field(description=\"The name of the character.\")\n",
    "    occupation: str = Field(description=\"The occupation of the character.\")\n",
    "    species: Literal[\"Human\", \"Insectoid\", \"Void-Serpent\", \"Synth\", \"Ethereal\", \"Liquid-Metal\"] = Field(\n",
    "        description=\"The race of the character.\"\n",
    "    )\n",
    "    back_story: str = Field(description=\"Brief backstory of this character.\")\n",
    "\n",
    "\n",
    "user_message = UserMessage(\n",
    "    \"Create a fantasy sci-fi character for my new game. This character will be the main protagonist, be creative.\"\n",
    ")\n",
    "\n",
    "response = await model.create_structure(\n",
    "    {\n",
    "        \"schema\": CharacterSchema,\n",
    "        \"messages\": [user_message],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts\n",
    "\n",
    "The SystemMessage is a special message type that can influence the general behavior of an LLM. By including a SystemMessage, you can provide high-level instructions that shape the LLM’s overall response style. The system message typically appears as the first message in the model’s memory.\n",
    "\n",
    "In the example below, we add a system message that instructs the LLM to speak like a pirate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr matey, ye be askin' about wee creatures, aye? A baby hedgehog be known as a \"hoglet\" in the tongue of the landlubbers. Now, that's a bit o' knowledge for yer next treasure hunt!\n"
     ]
    }
   ],
   "source": [
    "from beeai_framework.backend.message import SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"You are pirate. You always respond using pirate slang.\")\n",
    "user_message = UserMessage(content=\"What is a baby hedgehog called?\")\n",
    "output: ChatModelOutput = await model.create(ChatModelInput(messages=[system_message, user_message]))\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Agent\n",
    "\n",
    "You’re now ready to build your first agent! Proceed to the [workflows.ipynb](workflows.ipynb) notebook to continue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
